{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 376/376\r"
     ]
    }
   ],
   "source": [
    "# Scrape the JUPAS website\n",
    "url_JUPAS = 'https://www.jupas.edu.hk/en/programmes-offered/by-funding-category/'\n",
    "response_JUPAS = requests.get(url_JUPAS)\n",
    "soup_JUPAS = BeautifulSoup(response_JUPAS.content, 'html.parser')\n",
    "\n",
    "# Extract all the university abbreviations from the class 'schools_container'\n",
    "# The \"a\" elements are inside the div with the class 'schools_container'\n",
    "# The university abbreviations are the last part of the URL\n",
    "university_abbrs = []\n",
    "for div in soup_JUPAS.find_all('div', class_='schools_container'):\n",
    "    for a in div.find_all('a'):\n",
    "        university_abbrs.append(a['href'].split('/')[-1])\n",
    "\n",
    "# Replace 'programme-information' with 'sssdp'\n",
    "university_abbrs = [abbr.replace('programme-information', 'sssdp') for abbr in university_abbrs]\n",
    "\n",
    "\n",
    "# Scrape the programme information for each university\n",
    "# The URL for the programme list is https://www.jupas.edu.hk/en/programme/{university_abbr}/\n",
    "# Loop through each university abbreviation\n",
    "\n",
    "university_data = []\n",
    "\n",
    "for university in university_abbrs:\n",
    "    # Create the URL\n",
    "    url_uni = 'https://www.jupas.edu.hk/en/programme/' + university + '/'\n",
    "\n",
    "    # Scrape the website\n",
    "    response_uni = requests.get(url_uni)\n",
    "    soup_uni = BeautifulSoup(response_uni.content, 'html.parser')\n",
    "\n",
    "    # Extract the table from the website\n",
    "    table = soup_uni.find('table', class_='program_table program_table-hasFC')\n",
    "\n",
    "    # Extract the column names from the table\n",
    "    column_names = [th.text for th in table.find_all('th')]\n",
    "\n",
    "    # Add the column names for the url and Chinese name\n",
    "    column_names.append('chinese_name')\n",
    "    column_names.append('url')\n",
    "\n",
    "    # Create a list of dictionaries to store the data\n",
    "    datalist = []\n",
    "\n",
    "    # Extract the data from the table\n",
    "    # Include the url of the programme in the DataFrame in url column. The url is in the <a> tag within the <td> tage with class 'c-no'\n",
    "\n",
    "    for tr in table.find_all('tr'):\n",
    "        # Ignore the first row of the table, which is the column names\n",
    "        if tr.find('th'):\n",
    "            continue\n",
    "        \n",
    "        # For the column \"Programme Full Title\", the English name is the text of the <td> tag with class=\"c-ft\". The Chinese name is the text of the <span> tag with class=\"tname-cn\". Ignore the class=\"label\" <span> tag.\n",
    "        # Separate the text into Chinese and English into two columns, with the English name in the \"Programme Full Title\" column and the Chinese name in the \"Chinese Name\" column.\n",
    "        \n",
    "        # Extract the data from each row\n",
    "        row = [td.text for td in tr.find_all('td')]\n",
    "\n",
    "        # Extract the English name of the programme and replace the English name with the Full Title in the row\n",
    "        english_name = tr.find('td', class_='c-ft').contents[0].strip()\n",
    "        row[-1] = english_name\n",
    "\n",
    "        # Extract the url of the programme\n",
    "        url = tr.find('td', class_='c-no').find('a')['href']\n",
    "\n",
    "        # Extract the Chinese name of the programme\n",
    "        chinese_name = tr.find('td', class_='c-ft').find('span', class_='tname-cn').text\n",
    "\n",
    "        # Add the url and Chinese name to the row\n",
    "        row.append(chinese_name)\n",
    "        row.append(url)\n",
    "\n",
    "        # Create a dictionary from the row\n",
    "        data = dict(zip(column_names, row))\n",
    "\n",
    "        # Add the dictionary to the list\n",
    "        datalist.append(data)\n",
    "\n",
    "    # Add the list of dictionaries to the university_data list\n",
    "    university_data.extend(datalist)\n",
    "\n",
    "    # Add a progress counter to check the progress\n",
    "    print(f'Progress: {len(university_data)}/376', end='\\r')\n",
    "\n",
    "df_all = pd.DataFrame(university_data).set_index('JUPAS Catalogue No.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_excel('2024 JUPAS Program Overview.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offer_table = pd.DataFrame()\n",
    "for program in university_data:\n",
    "\n",
    "    print(f'Currently Scraping: {index}, Progress: {count+1}/{df_all.shape[0]}', end='\\r')\n",
    "    \n",
    "    # URL to scape:\n",
    "    url_programme = 'https://www.jupas.edu.hk/' + row['url']\n",
    "\n",
    "    # Which school\n",
    "    school = row['Institution / Scheme']\n",
    "\n",
    "    # Scrape the website\n",
    "    response_programme = requests.get(url_programme)\n",
    "    soup_programme = BeautifulSoup(response_programme.content, 'html.parser')\n",
    "\n",
    "    # Getting the quota\n",
    "    quota = soup_programme.find('div', class_='programInfo_block programInfo_block-firstyear').text.strip()\n",
    "    quota = re.sub(r'\\D', '', quota)\n",
    "\n",
    "\n",
    "    title = soup_programme.find('p', class_='strokeBar_title', string=\"Statistics\")\n",
    "\n",
    "    if title:\n",
    "        div = title.find_parent('div', class_='strokeBar_box')   \n",
    "\n",
    "        tables = div.find_all('table', class_='js-swrapTable program_brand_table js-swiptable statistic-table')\n",
    "\n",
    "        a_stat = []\n",
    "        o_stat = []\n",
    "\n",
    "        for table in tables:\n",
    "\n",
    "            # Extract the table body\n",
    "            table_body = table.find('tbody').find_all('tr')\n",
    "            table_rows = [[item.text.strip() for item in rows.find_all('td')] for rows in table_body]\n",
    "\n",
    "            header = table_rows[0]\n",
    "            formatted_data = []\n",
    "\n",
    "            for row in table_rows[1:]:\n",
    "                formatted_row = dict(zip(header, row))\n",
    "                formatted_data.append(formatted_row)\n",
    "            \n",
    "            # In this loop, we will have two tables\n",
    "            # Append the first table to the Application Statistics\n",
    "            # Append the second table to the Offer Statistics\n",
    "            if tables.index(table) == 0:\n",
    "                a_stat = formatted_data\n",
    "            else:\n",
    "                o_stat= formatted_data\n",
    "\n",
    "        a_df=pd.DataFrame.from_dict(a_stat)\n",
    "        a_df['JUPAS'] = index\n",
    "        a_df['Type'] = \"Application\"\n",
    "        a_df['School'] = school\n",
    "        a_df['Quota'] = quota\n",
    "\n",
    "        o_df=pd.DataFrame.from_dict(o_stat)\n",
    "        o_df['JUPAS'] = index\n",
    "        o_df['Type'] = \"Offer\"\n",
    "        o_df['School'] = school\n",
    "        o_df['Quota'] = quota\n",
    "\n",
    "        combined_df = pd.concat([a_df,o_df], ignore_index=True)\n",
    "        offer_table = pd.concat([offer_table,combined_df], ignore_index=True)\n",
    "    else:\n",
    "        new_row = {'JUPAS':index, 'School':school,'Quota':quota}\n",
    "        \n",
    "        offer_table= pd.concat([offer_table, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "#offer_table.to_excel('2024 JUPAS Offer Table.xlsx', index=False)\n",
    "offer_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently Scraping: JS1108, Progress: 30/376\r"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m soup_programme \u001b[38;5;241m=\u001b[39m BeautifulSoup(response_programme\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Getting the quota\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m quota \u001b[38;5;241m=\u001b[39m soup_programme\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprogramInfo_block programInfo_block-firstyear\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     18\u001b[0m quota \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, quota)\n\u001b[0;32m     21\u001b[0m title \u001b[38;5;241m=\u001b[39m soup_programme\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrokeBar_title\u001b[39m\u001b[38;5;124m'\u001b[39m, string\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatistics\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "offer_table = pd.DataFrame()\n",
    "for count,program in enumerate(university_data):\n",
    "\n",
    "    print(f'Currently Scraping: {program[\"JUPAS Catalogue No.\"]}, Progress: {count+1}/{len(university_data)}', end='\\r')\n",
    "    \n",
    "    # URL to scape:\n",
    "    url_programme = 'https://www.jupas.edu.hk/' + program['url']\n",
    "\n",
    "    # Which school\n",
    "    school = program['Institution / Scheme']\n",
    "\n",
    "    # Scrape the website\n",
    "    response_programme = requests.get(url_programme)\n",
    "    soup_programme = BeautifulSoup(response_programme.content, 'html.parser')\n",
    "\n",
    "    # Getting the quota\n",
    "    quota = soup_programme.find('div', class_='programInfo_block programInfo_block-firstyear').text.strip()\n",
    "    quota = re.sub(r'\\D', '', quota)\n",
    "\n",
    "\n",
    "    title = soup_programme.find('p', class_='strokeBar_title', string=\"Statistics\")\n",
    "\n",
    "    if title:\n",
    "        div = title.find_parent('div', class_='strokeBar_box')   \n",
    "\n",
    "        tables = div.find_all('table', class_='js-swrapTable program_brand_table js-swiptable statistic-table')\n",
    "\n",
    "        a_stat = []\n",
    "        o_stat = []\n",
    "\n",
    "        for table in tables:\n",
    "\n",
    "            # Extract the table body\n",
    "            table_body = table.find('tbody').find_all('tr')\n",
    "            table_rows = [[item.text.strip() for item in rows.find_all('td')] for rows in table_body]\n",
    "\n",
    "            header = table_rows[0]\n",
    "            formatted_data = []\n",
    "\n",
    "            for row in table_rows[1:]:\n",
    "                formatted_row = dict(zip(header, row))\n",
    "                formatted_data.append(formatted_row)\n",
    "            \n",
    "            # In this loop, we will have two tables\n",
    "            # Append the first table to the Application Statistics\n",
    "            # Append the second table to the Offer Statistics\n",
    "            if tables.index(table) == 0:\n",
    "                a_stat = formatted_data\n",
    "            else:\n",
    "                o_stat= formatted_data\n",
    "\n",
    "        a_df=pd.DataFrame.from_dict(a_stat)\n",
    "        a_df['JUPAS'] = program[\"JUPAS Catalogue No.\"]\n",
    "        a_df['Type'] = \"Application\"\n",
    "        a_df['School'] = school\n",
    "        a_df['Quota'] = quota\n",
    "\n",
    "        o_df=pd.DataFrame.from_dict(o_stat)\n",
    "        o_df['JUPAS'] = program[\"JUPAS Catalogue No.\"]\n",
    "        o_df['Type'] = \"Offer\"\n",
    "        o_df['School'] = school\n",
    "        o_df['Quota'] = quota\n",
    "\n",
    "        combined_df = pd.concat([a_df,o_df], ignore_index=True)\n",
    "        offer_table = pd.concat([offer_table,combined_df], ignore_index=True)\n",
    "    else:\n",
    "        new_row = {'JUPAS':program[\"JUPAS Catalogue No.\"], 'School':school,'Quota':quota}\n",
    "        \n",
    "        offer_table= pd.concat([offer_table, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "offer_table.to_excel('2024 JUPAS Offer Table.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
